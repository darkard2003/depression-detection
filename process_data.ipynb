{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwtnWQJ29cdT"
   },
   "source": [
    "# Data Processing Pipeline for Depression Detection\n",
    "\n",
    "This notebook processes Reddit text data for depression detection using:\n",
    "- Text preprocessing (cleaning, tokenization, lemmatization)\n",
    "- TF-IDF vectorization (5000 features)\n",
    "- NRCLex emotion extraction (10 features)\n",
    "- Feature combination and normalization\n",
    "\n",
    "**Input**: `bin_reddit1.csv` (raw Reddit posts)  \n",
    "**Output**: Processed feature matrices saved to `processed/` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IT6cLk3nhvsp",
    "outputId": "fce588f9-e321-4349-cc5f-a33eb1209491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ wordsegment not installed - using regex-based hashtag processing\n",
      "  To enable advanced processing: pip install wordsegment\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nrclex import NRCLex\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "import os\n",
    "\n",
    "# Try to import wordsegment for advanced hashtag processing\n",
    "try:\n",
    "    from wordsegment import load, segment\n",
    "    load()\n",
    "    WORDSEGMENT_AVAILABLE = True\n",
    "    print(\"✓ wordsegment library available - using advanced hashtag processing\")\n",
    "except ImportError:\n",
    "    WORDSEGMENT_AVAILABLE = False\n",
    "    print(\"⚠ wordsegment not installed - using regex-based hashtag processing\")\n",
    "    print(\"  To enable advanced processing: pip install wordsegment\")\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Install wordsegment for better hashtag processing\n",
    "\n",
    "For improved hashtag segmentation (e.g., `#mentalhealth` → \"mental health\"), install the `wordsegment` library:\n",
    "\n",
    "```bash\n",
    "pip install wordsegment\n",
    "```\n",
    "\n",
    "Without it, the notebook will use regex-based processing which still works well for camelCase and underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LytMLckZcB_O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resources...\n",
      "NLTK resources downloaded successfully!\n",
      "NLTK resources downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "print(\"Downloading NLTK resources...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "print(\"NLTK resources downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "M5MIrWzn9t1-",
    "outputId": "f0979a20-f7ad-4847-a5b6-d3dd861056d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (99590, 3)\n",
      "\n",
      "Column names: ['text', 'label', ' ']\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0    58405\n",
      "1    41185\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa glad fun paint night sky</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandonment massive fear trigger suicidal</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ability induce anxiety gift god</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ability write complex business</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text  label    \n",
       "0                  aa glad fun paint night sky      0 NaN\n",
       "1    abandonment massive fear trigger suicidal      1 NaN\n",
       "2              ability induce anxiety gift god      0 NaN\n",
       "3               ability write complex business      0 NaN\n",
       "4                                            Q      0 NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('bin_reddit1.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hashtag processing:\n",
      "Mode: Regex-based processing\n",
      "\n",
      "  #MentalHealth                       → mental health\n",
      "  #mentalhealth                       → mentalhealth\n",
      "  #mental_health                      → mental health\n",
      "  #COVID19                            → covid 19\n",
      "  #MentalHealthAwareness2024          → mental health awareness 2024\n",
      "  #depressed                          → depressed\n",
      "  #IFeelDepressed                     → i feel depressed\n",
      "  #2024Goals                          → 2024 goals\n",
      "  #selfcare                           → selfcare\n",
      "\n",
      "✓ Hashtag processing function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def process_hashtags(text):\n",
    "    \"\"\"\n",
    "    Advanced hashtag processing with multiple strategies.\n",
    "    \n",
    "    If wordsegment is available (pip install wordsegment):\n",
    "        Uses statistical word segmentation to split hashtags intelligently\n",
    "        Example: #mentalhealth → \"mental health\" (automatically recognizes words)\n",
    "    \n",
    "    Otherwise uses regex-based approach:\n",
    "        1. Split camelCase: #MentalHealth → mental health\n",
    "        2. Replace underscores: #mental_health → mental health\n",
    "        3. Separate numbers: #covid19 → covid 19\n",
    "        4. Handle mixed case: #MentalHealthAwareness2024 → mental health awareness 2024\n",
    "    \n",
    "    Examples:\n",
    "        #MentalHealth → \"mental health\"\n",
    "        #mentalhealth → \"mental health\" (with wordsegment) or \"mentalhealth\" (without)\n",
    "        #mental_health → \"mental health\"\n",
    "        #COVID19 → \"covid 19\"\n",
    "        #MentalHealthAwareness2024 → \"mental health awareness 2024\"\n",
    "    \"\"\"\n",
    "    hashtags = re.findall(r'#(\\w+)', text)\n",
    "    \n",
    "    for hashtag in hashtags:\n",
    "        if WORDSEGMENT_AVAILABLE:\n",
    "            # Statistical word segmentation (best approach)\n",
    "            # Automatically splits concatenated words: \"mentalhealth\" → [\"mental\", \"health\"]\n",
    "            words = segment(hashtag.lower())\n",
    "            processed = ' '.join(words)\n",
    "        else:\n",
    "            # Fallback: regex-based processing\n",
    "            processed = hashtag\n",
    "            \n",
    "            # Step 1: Replace underscores with spaces\n",
    "            processed = processed.replace('_', ' ')\n",
    "            \n",
    "            # Step 2: Split camelCase (lowercase followed by uppercase)\n",
    "            # Handles: MentalHealth → Mental Health\n",
    "            processed = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', processed)\n",
    "            \n",
    "            # Step 3: Split on uppercase sequences followed by lowercase\n",
    "            # Handles: MENTALHealth → MENTAL Health\n",
    "            processed = re.sub(r'(?<=[A-Z])(?=[A-Z][a-z])', ' ', processed)\n",
    "            \n",
    "            # Step 4: Separate numbers from letters\n",
    "            # Handles: covid19 → covid 19, 2024awareness → 2024 awareness\n",
    "            processed = re.sub(r'(?<=[a-zA-Z])(?=\\d)', ' ', processed)\n",
    "            processed = re.sub(r'(?<=\\d)(?=[a-zA-Z])', ' ', processed)\n",
    "            \n",
    "            # Step 5: Convert to lowercase and clean up extra spaces\n",
    "            processed = ' '.join(processed.lower().split())\n",
    "        \n",
    "        # Replace in original text\n",
    "        text = text.replace(f'#{hashtag}', processed)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the function with examples\n",
    "print(\"Testing hashtag processing:\")\n",
    "print(f\"Mode: {'Statistical word segmentation (wordsegment)' if WORDSEGMENT_AVAILABLE else 'Regex-based processing'}\")\n",
    "print()\n",
    "\n",
    "test_cases = [\n",
    "    \"#MentalHealth\",\n",
    "    \"#mentalhealth\",  # Only wordsegment handles this well\n",
    "    \"#mental_health\",\n",
    "    \"#COVID19\",\n",
    "    \"#MentalHealthAwareness2024\",\n",
    "    \"#depressed\",\n",
    "    \"#IFeelDepressed\",\n",
    "    \"#2024Goals\",\n",
    "    \"#selfcare\"  # Another test for wordsegment\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    result = process_hashtags(test)\n",
    "    print(f\"  {test:35} → {result}\")\n",
    "\n",
    "print(\"\\n✓ Hashtag processing function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "71FmJWPE-BGA",
    "outputId": "a221b49a-6931-4ed7-cb95-980f962589b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text preprocessing pipeline...\n",
      "1/6 - Removing URLs...\n",
      "2/6 - Removing mentions...\n",
      "3/6 - Converting to lowercase...\n",
      "4/6 - Removing numbers...\n",
      "5/6 - Converting emojis to text...\n",
      "6/6 - Processing hashtags...\n",
      "\n",
      "✓ Text preprocessing complete!\n",
      "\n",
      "Sample outputs:\n",
      "                                text_for_tfidf  \\\n",
      "0                  aa glad fun paint night sky   \n",
      "1    abandonment massive fear trigger suicidal   \n",
      "2              ability induce anxiety gift god   \n",
      "3               ability write complex business   \n",
      "4                                            q   \n",
      "\n",
      "                                  text_for_nrc  label  \n",
      "0                  aa glad fun paint night sky      0  \n",
      "1    abandonment massive fear trigger suicidal      1  \n",
      "2              ability induce anxiety gift god      0  \n",
      "3               ability write complex business      0  \n",
      "4                                            q      0  \n",
      "6/6 - Processing hashtags...\n",
      "\n",
      "✓ Text preprocessing complete!\n",
      "\n",
      "Sample outputs:\n",
      "                                text_for_tfidf  \\\n",
      "0                  aa glad fun paint night sky   \n",
      "1    abandonment massive fear trigger suicidal   \n",
      "2              ability induce anxiety gift god   \n",
      "3               ability write complex business   \n",
      "4                                            q   \n",
      "\n",
      "                                  text_for_nrc  label  \n",
      "0                  aa glad fun paint night sky      0  \n",
      "1    abandonment massive fear trigger suicidal      1  \n",
      "2              ability induce anxiety gift god      0  \n",
      "3               ability write complex business      0  \n",
      "4                                            q      0  \n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer (not used for NRCLex)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Starting text preprocessing pipeline...\")\n",
    "\n",
    "# Step 1: Remove URLs\n",
    "print(\"1/6 - Removing URLs...\")\n",
    "df['text'] = df['text'].str.replace(r'http\\S+', '', regex=True)\n",
    "\n",
    "# Step 2: Remove mentions\n",
    "print(\"2/6 - Removing mentions...\")\n",
    "df['text'] = df['text'].str.replace(r'@\\S+', '', regex=True)\n",
    "\n",
    "# Step 3: Normalize to lowercase\n",
    "print(\"3/6 - Converting to lowercase...\")\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# Step 4: Remove numbers\n",
    "print(\"4/6 - Removing numbers...\")\n",
    "df['text'] = df['text'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "# Step 5: Replace emojis with text\n",
    "print(\"5/6 - Converting emojis to text...\")\n",
    "df['text'] = df['text'].apply(lambda x: emoji.demojize(x, delimiters=(\" \", \" \")))\n",
    "\n",
    "# Step 6: Convert hashtags to normal text\n",
    "print(\"6/6 - Processing hashtags...\")\n",
    "df['text'] = df['text'].apply(process_hashtags)\n",
    "\n",
    "# Save intermediate texts for feature extractors\n",
    "# TF-IDF works best with the raw cleaned text (let it tokenize itself)\n",
    "df['text_for_tfidf'] = df['text'].copy()\n",
    "# NRCLex works better when negations/stopwords are preserved (no lemmatization)\n",
    "df['text_for_nrc'] = df['text'].copy()\n",
    "\n",
    "print(\"\\n✓ Text preprocessing complete!\")\n",
    "print(f\"\\nSample outputs:\")\n",
    "print(df[['text_for_tfidf', 'text_for_nrc', 'label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Text Preprocessing Pipeline\n",
    "\n",
    "Steps:\n",
    "1. Remove URLs and mentions\n",
    "2. Convert to lowercase\n",
    "3. Remove numbers\n",
    "4. Convert emojis to text\n",
    "5. Process hashtags\n",
    "6. Create two text streams:\n",
    "   - text_for_tfidf (cleaned, not tokenized)\n",
    "   - text_for_nrc (negations/stopwords preserved)\n",
    "\n",
    "Note: We no longer tokenize/lemmatize for NRCLex to preserve cues like negation; TF-IDF handles its own tokenization and stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "057eb280",
    "outputId": "1e3aab81-24f0-4524-91d5-64a2925819e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS DISTRIBUTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Class 0 (Non-depression): 58405 samples\n",
      "Class 1 (Depression): 41185 samples\n",
      "\n",
      "Imbalance ratio: 1.42:1\n",
      "\n",
      "Sample from Class 0:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_for_nrc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa glad fun paint night sky</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ability induce anxiety gift god</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ability write complex business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        text_for_nrc  label\n",
       "0        aa glad fun paint night sky      0\n",
       "2    ability induce anxiety gift god      0\n",
       "3     ability write complex business      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample from Class 1:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_for_nrc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandonment massive fear trigger suicidal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>absence mental illness doesnt presence menta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>absolute bastard odd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_for_nrc  label\n",
       "1           abandonment massive fear trigger suicidal      1\n",
       "7     absence mental illness doesnt presence menta...      1\n",
       "11                               absolute bastard odd      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze class distribution\n",
    "df_class_0 = df[df['label'] == 0]\n",
    "df_class_1 = df[df['label'] == 1]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nClass 0 (Non-depression): {df_class_0.shape[0]} samples\")\n",
    "print(f\"Class 1 (Depression): {df_class_1.shape[0]} samples\")\n",
    "print(f\"\\nImbalance ratio: {df_class_0.shape[0] / df_class_1.shape[0]:.2f}:1\")\n",
    "print(\"\\nSample from Class 0:\")\n",
    "display(df_class_0[['text_for_nrc', 'label']].head(3))\n",
    "print(\"\\nSample from Class 1:\")\n",
    "display(df_class_1[['text_for_nrc', 'label']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction\n",
    "\n",
    "### 6.1 TF-IDF Features (5000 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5a9b2751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 99590\n",
      "TF-IDF input: (99590,)\n",
      "NRCLex input: (99590,)\n",
      "Target (y): (99590,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "X_tfidf_text = df['text_for_tfidf']   # Use mid-cleaned text (no tokenization) for TF-IDF\n",
    "X_nrc_text = df['text_for_nrc']       # Use mid-cleaned text (preserve negations/stopwords) for NRCLex\n",
    "y = df['label']\n",
    "\n",
    "print(f\"Total samples: {len(y)}\")\n",
    "print(f\"TF-IDF input: {X_tfidf_text.shape}\")\n",
    "print(f\"NRCLex input: {X_nrc_text.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de3420aa",
    "outputId": "72273bfd-7d50-4168-9944-5db614ef3b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting TF-IDF features...\n",
      "Configuration:\n",
      "  - Using text_for_tfidf (cleaned but not tokenized)\n",
      "  - TF-IDF handles tokenization, stop words, and n-grams internally\n",
      "\n",
      "✓ TF-IDF shape: (99590, 5000)\n",
      "  - Sparse matrix with 662,785 non-zero elements\n",
      "  - Sparsity: 99.87%\n",
      "  - Vocabulary size: 5,000 unique terms\n",
      "\n",
      "✓ TF-IDF shape: (99590, 5000)\n",
      "  - Sparse matrix with 662,785 non-zero elements\n",
      "  - Sparsity: 99.87%\n",
      "  - Vocabulary size: 5,000 unique terms\n"
     ]
    }
   ],
   "source": [
    "# Extract TF-IDF features with optimized configuration\n",
    "print(\"Extracting TF-IDF features...\")\n",
    "print(\"Configuration:\")\n",
    "print(\"  - Using text_for_tfidf (cleaned but not tokenized)\")\n",
    "print(\"  - TF-IDF handles tokenization, stop words, and n-grams internally\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),          # Include unigrams and bigrams\n",
    "    min_df=2,                    # Ignore terms that appear in fewer than 2 documents\n",
    "    max_df=0.95,                 # Ignore terms that appear in more than 95% of documents\n",
    "    stop_words='english',        # Let TF-IDF handle stop words removal\n",
    "    token_pattern=r'\\b\\w+\\b',    # Match word tokens\n",
    "    strip_accents='unicode',     # Normalize unicode characters\n",
    "    lowercase=True,              # Already lowercased, but ensure consistency\n",
    "    sublinear_tf=True           # Use log(tf) instead of raw frequency\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X_tfidf_text)\n",
    "\n",
    "print(f\"\\n✓ TF-IDF shape: {X_tfidf.shape}\")\n",
    "print(f\"  - Sparse matrix with {X_tfidf.nnz:,} non-zero elements\")\n",
    "print(f\"  - Sparsity: {(1 - X_tfidf.nnz / (X_tfidf.shape[0] * X_tfidf.shape[1])) * 100:.2f}%\")\n",
    "print(f\"  - Vocabulary size: {len(tfidf_vectorizer.vocabulary_):,} unique terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 NRCLex Emotion Features (10 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "c4273751",
    "outputId": "5babcc89-049a-4664-b584-caa31d3208fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting NRCLex emotion features...\n",
      "Using mid-cleaned text_for_nrc (negations preserved, no lemmatization)\n",
      "  Processing: 5000/99590 texts...\n",
      "  Processing: 10000/99590 texts...\n",
      "  Processing: 15000/99590 texts...\n",
      "  Processing: 10000/99590 texts...\n",
      "  Processing: 15000/99590 texts...\n",
      "  Processing: 20000/99590 texts...\n",
      "  Processing: 25000/99590 texts...\n",
      "  Processing: 20000/99590 texts...\n",
      "  Processing: 25000/99590 texts...\n",
      "  Processing: 30000/99590 texts...\n",
      "  Processing: 35000/99590 texts...\n",
      "  Processing: 30000/99590 texts...\n",
      "  Processing: 35000/99590 texts...\n",
      "  Processing: 40000/99590 texts...\n",
      "  Processing: 45000/99590 texts...\n",
      "  Processing: 40000/99590 texts...\n",
      "  Processing: 45000/99590 texts...\n",
      "  Processing: 50000/99590 texts...\n",
      "  Processing: 55000/99590 texts...\n",
      "  Processing: 50000/99590 texts...\n",
      "  Processing: 55000/99590 texts...\n",
      "  Processing: 60000/99590 texts...\n",
      "  Processing: 65000/99590 texts...\n",
      "  Processing: 60000/99590 texts...\n",
      "  Processing: 65000/99590 texts...\n",
      "  Processing: 70000/99590 texts...\n",
      "  Processing: 75000/99590 texts...\n",
      "  Processing: 70000/99590 texts...\n",
      "  Processing: 75000/99590 texts...\n",
      "  Processing: 80000/99590 texts...\n",
      "  Processing: 85000/99590 texts...\n",
      "  Processing: 80000/99590 texts...\n",
      "  Processing: 85000/99590 texts...\n",
      "  Processing: 90000/99590 texts...\n",
      "  Processing: 95000/99590 texts...\n",
      "  Processing: 90000/99590 texts...\n",
      "  Processing: 95000/99590 texts...\n",
      "\n",
      "✓ NRC features shape: (99590, 10)\n",
      "\n",
      "Emotion columns:\n",
      "['anticipation', 'joy', 'positive', 'anger', 'fear', 'negative', 'sadness', 'surprise', 'disgust', 'trust']\n",
      "\n",
      "Sample emotion scores:\n",
      "\n",
      "✓ NRC features shape: (99590, 10)\n",
      "\n",
      "Emotion columns:\n",
      "['anticipation', 'joy', 'positive', 'anger', 'fear', 'negative', 'sadness', 'surprise', 'disgust', 'trust']\n",
      "\n",
      "Sample emotion scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anticipation</th>\n",
       "      <th>joy</th>\n",
       "      <th>positive</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>disgust</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   anticipation  joy  positive  anger  fear  negative  sadness  surprise  \\\n",
       "0           2.0  2.0       3.0    0.0   0.0       0.0      0.0       0.0   \n",
       "1           0.0  0.0       0.0    3.0   3.0       3.0      2.0       1.0   \n",
       "2           3.0  2.0       3.0    1.0   2.0       1.0      1.0       1.0   \n",
       "3           0.0  0.0       1.0    0.0   0.0       0.0      0.0       0.0   \n",
       "4           0.0  0.0       0.0    0.0   0.0       0.0      0.0       0.0   \n",
       "\n",
       "   disgust  trust  \n",
       "0      0.0    0.0  \n",
       "1      1.0    0.0  \n",
       "2      0.0    1.0  \n",
       "3      0.0    0.0  \n",
       "4      0.0    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract NRC emotion features\n",
    "def extract_nrc_features(text_list):\n",
    "    \"\"\"\n",
    "    Extract emotion features using NRCLex.\n",
    "    Returns a DataFrame with emotion scores for each text.\n",
    "    \"\"\"\n",
    "    nrc_features = []\n",
    "    total = len(text_list)\n",
    "    \n",
    "    for i, text in enumerate(text_list):\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"  Processing: {i + 1}/{total} texts...\")\n",
    "        \n",
    "        emotion_object = NRCLex(text)\n",
    "        scores = emotion_object.raw_emotion_scores\n",
    "        nrc_features.append(scores)\n",
    "    \n",
    "    # Convert to DataFrame and fill missing values\n",
    "    nrc_df = pd.DataFrame(nrc_features).fillna(0)\n",
    "    return nrc_df\n",
    "\n",
    "print(\"Extracting NRCLex emotion features...\")\n",
    "print(\"Using mid-cleaned text_for_nrc (negations preserved, no lemmatization)\")\n",
    "text_list = X_nrc_text.tolist()\n",
    "X_nrc_features = extract_nrc_features(text_list)\n",
    "\n",
    "print(f\"\\n✓ NRC features shape: {X_nrc_features.shape}\")\n",
    "print(f\"\\nEmotion columns:\")\n",
    "print(X_nrc_features.columns.tolist())\n",
    "print(f\"\\nSample emotion scores:\")\n",
    "display(X_nrc_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Normalize NRC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUAf-p-zrv3l",
    "outputId": "aef806e0-0d03-482d-a712-3c293538224c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing NRC features...\n",
      "✓ Scaled NRC features shape: (99590, 10)\n",
      "  - Min value: 0.0\n",
      "  - Max value: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalize NRC features to [0, 1] range\n",
    "print(\"Normalizing NRC features...\")\n",
    "scaler = MinMaxScaler()\n",
    "X_nrc_features_scaled = scaler.fit_transform(X_nrc_features)\n",
    "\n",
    "print(f\"✓ Scaled NRC features shape: {X_nrc_features_scaled.shape}\")\n",
    "print(f\"  - Min value: {X_nrc_features_scaled.min()}\")\n",
    "print(f\"  - Max value: {X_nrc_features_scaled.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data\n",
    "\n",
    "Save all feature matrices and target variable to the `processed/` folder for use in model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Combine TF-IDF and NRC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining TF-IDF and NRC features...\n",
      "✓ Combined features shape: (99590, 5010)\n",
      "  - TF-IDF: 5000 features\n",
      "  - NRC: 10 features\n",
      "  - Total: 5010 features\n"
     ]
    }
   ],
   "source": [
    "# Combine TF-IDF and NRC features efficiently (keep TF-IDF sparse)\n",
    "print(\"Combining TF-IDF and NRC features...\")\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Convert scaled NRC features (dense) to sparse for efficient concatenation\n",
    "nrc_sparse = csr_matrix(X_nrc_features_scaled)\n",
    "\n",
    "# Horizontally stack sparse TF-IDF with sparse NRC features\n",
    "X_combined_sparse = hstack([X_tfidf, nrc_sparse], format='csr')\n",
    "\n",
    "print(f\"✓ Combined features shape: {X_combined_sparse.shape}\")\n",
    "print(f\"  - TF-IDF: {X_tfidf.shape[1]} features\")\n",
    "print(f\"  - NRC: {nrc_sparse.shape[1]} features\")\n",
    "print(f\"  - Total: {X_combined_sparse.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b47f1cfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving processed data...\n",
      "✓ Saved: X_tfidf.npz ((99590, 5000))\n",
      "✓ Saved: X_nrc_features_scaled.npy ((99590, 10))\n",
      "✓ Saved: X_tfidf.npz ((99590, 5000))\n",
      "✓ Saved: X_nrc_features_scaled.npy ((99590, 10))\n",
      "✓ Saved: X_combined_sparse.npz ((99590, 5010))\n",
      "✓ Saved: y.npy ((99590,))\n",
      "\n",
      "============================================================\n",
      "DATA PROCESSING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "All files saved to: processed/\n",
      "\n",
      "Generated files:\n",
      "  1. X_tfidf.npz              - TF-IDF features only (5000 features)\n",
      "  2. X_nrc_features_scaled.npy - NRCLex features only (10 features)\n",
      "  3. X_combined_sparse.npz    - Combined features (5010 features)\n",
      "  4. y.npy                    - Target labels\n",
      "✓ Saved: X_combined_sparse.npz ((99590, 5010))\n",
      "✓ Saved: y.npy ((99590,))\n",
      "\n",
      "============================================================\n",
      "DATA PROCESSING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "All files saved to: processed/\n",
      "\n",
      "Generated files:\n",
      "  1. X_tfidf.npz              - TF-IDF features only (5000 features)\n",
      "  2. X_nrc_features_scaled.npy - NRCLex features only (10 features)\n",
      "  3. X_combined_sparse.npz    - Combined features (5010 features)\n",
      "  4. y.npy                    - Target labels\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "save_folder = 'processed'\n",
    "\n",
    "# Create save folder if it doesn't exist\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "    print(f\"Created folder: {save_folder}/\")\n",
    "\n",
    "print(\"\\nSaving processed data...\")\n",
    "\n",
    "# Save TF-IDF features (sparse matrix)\n",
    "save_npz(os.path.join(save_folder, 'X_tfidf.npz'), X_tfidf)\n",
    "print(f\"✓ Saved: X_tfidf.npz ({X_tfidf.shape})\")\n",
    "\n",
    "# Save scaled NRC features (dense array)\n",
    "np.save(os.path.join(save_folder, 'X_nrc_features_scaled.npy'), X_nrc_features_scaled)\n",
    "print(f\"✓ Saved: X_nrc_features_scaled.npy ({X_nrc_features_scaled.shape})\")\n",
    "\n",
    "# Save combined features (sparse matrix)\n",
    "save_npz(os.path.join(save_folder, 'X_combined_sparse.npz'), X_combined_sparse)\n",
    "print(f\"✓ Saved: X_combined_sparse.npz ({X_combined_sparse.shape})\")\n",
    "\n",
    "# Save target variable\n",
    "np.save(os.path.join(save_folder, 'y.npy'), y)\n",
    "print(f\"✓ Saved: y.npy ({y.shape})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA PROCESSING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAll files saved to: {save_folder}/\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. X_tfidf.npz              - TF-IDF features only (5000 features)\")\n",
    "print(\"  2. X_nrc_features_scaled.npy - NRCLex features only (10 features)\")\n",
    "print(\"  3. X_combined_sparse.npz    - Combined features (5010 features)\")\n",
    "print(\"  4. y.npy                    - Target labels\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "depression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
